if (length(arg1) == length(arg2)) {
print('Equal Length')
} else {
print("Not Equal Length")
}
}
compare("EPS568","Summer")
compare("EPS568","SummerA")
length("EPS568")
help("length")
compare = function(arg1, arg2) {
if (nchar(arg1) == nchar(arg2)) {
print('Equal Length')
} else {
print("Not Equal Length")
}
}
compare("EPS568","Summer")
compare("EPS568","SummerA")
length("EPS568")
strsplit(Names, split = ' ')
Names <- "John Andrew Thomas"
strsplit(Names, split = ' ')
paste0('list_N', Names)
paste0(list_N, '@gmail.com')
Names <- "John Andrew Thomas"
list_N <- strsplit(Names, split = ' ')
paste0(list_N, '@gmail.com')
paste(list_N, '@gmail.com')
list_N
list_N <- strsplit(Names, split = ' ')[[1]]
paste(list_N, '@gmail.com')
list_N <- strsplit(Names, split = '')[[1]]
paste(list_N, '@gmail.com')
paste(list_N, '@gmail.com')
Names <- "John Andrew Thomas"
list_N <- strsplit(Names, split = ' ')[[1]]
paste(list_N, '@gmail.com')
paste(list_N, '@gmail.com', sep = '')
# x3 = x2 + x1
i = 2
x = c(1, 1)
while (x[i] < 4e6) {
x[i + 1] = x[i - 1] + x[i]
i = i + 1
}
x = x[-i]
print(x)
a = 1:10
a[3]
a = 11:14
a[2]
# x3 = x2 + x1
i = 2
x = c(1, 1)
y = 0
while (x[i] < 4e6) {
x[i + 1] = x[i - 1] + x[i]
if (x[i] %% 2 == 0) {
y = y + x[i]
}
i = i + 1
}
x = x[-i]
print(y)
return(m)
Median_AD = function(vec) {
n = median(vec)
for (i in vec) {
m = median(i - n)
return(m)
}
}
Median_AD(1:15)
Median_AD = function(vec) {
n = median(vec)
for (i in vec) {
m = median(abs(i - n))
return(m)
}
}
Median_AD(1:15)
1:15
# x3 = x2 + x1
i = 2
x = c(1, 1)
y = 0
while (x[i] < 4e6) {
x[i + 1] = x[i - 1] + x[i]
if (x[i] %% 2 == 0) {
y = y + x[i]
}
i = i + 1
}
x = x[-i]
print(y)
for (row in 1:nrow(m)) {
for (col in 1:ncol(m)) {
print(m[row,col])
}
}
while (b < 4e6) {
if (b %% 2 == 0) {
summ = summ + b
}
temp = a + b
a = b
b = temp
}
a = 1
b = 2
summ = 0
while (b < 4e6) {
if (b %% 2 == 0) {
summ = summ + b
}
temp = a + b
a = b
b = temp
}
summ
mul_mat = function(mat, vec) {
for (row in 1:nrow(mat)) {
for (col in 1:ncol(mat)) {
for (i in vec) {
sum(mat[row,col]*vec[i])
} } } }
m = matrix(1:15, nrow = 3, ncol = 5)
n = seq(1,10, by = 2)
mul_mat(m,n)
mul_mat = function(mat, vec) {
for (row in 1:nrow(mat)) {
for (col in 1:ncol(mat)) {
for (i in vec) {
x = sum(mat[row,col]*vec[i])
print(x)
} } } }
m = matrix(1:15, nrow = 3, ncol = 5)
n = seq(1,10, by = 2)
mul_mat(m,n)
mul_mat = function(mat, vec) {
for (row in 1:nrow(mat)) {
for (col in 1:ncol(mat)) {
for (i in vec) {
return sum(mat[row,col]*vec[i])
mul_mat = function(mat, vec) {
sum = 0
for (row in 1:nrow(mat)) {
for (col in 1:ncol(mat)) {
for (i in vec) {
sum = sum + (mat[row,col]*vec[i])
} } }
sum
}
m = matrix(1:15, nrow = 3, ncol = 5)
n = seq(1,10, by = 2)
mul_mat(m,n)
m = median(abs(i - n)) + 1
return(m)
Median_AD = function(vec) {
n = median(vec)
for (i in vec) {
m = median(abs(i - n)) + 1
return(m)
}
}
Median_AD(1:15)
1:15
median(1:15)
Median_AD(1:15)
Mat_mul = function(mat, vec) {
c(mat%*%vec)
}
Median_AD = function(vec) {
n = median(vec)
median(abs(vec - n))
return(m)
}
}
Median_AD = function(vec) {
n = median(vec)
median(abs(vec - n))
return(m)
}
Median_AD(1:15)
Median_AD = function(vec) {
n = median(vec)
m = median(abs(vec - n))
return(m)
}
Median_AD(1:15)
Median_AD = function(vec) {
m = median(abs(vec - median(vec)))
return(m)
}
Median_AD(1:15)
Median_AD = function(vec) {
m = median(abs(vec - median(vec)))
return(m)
}
Median_AD(1:15)
Median_AD = function(vec) {
m = median(abs(vec - median(vec)))
return(m)
}
Median_AD(1:15)
Median_AD = function(vec) {
m = median(abs(vec - median(vec)))
return(m)
}
Median_AD(1:15)
Median_AD = function(vec) {
m = median(abs(vec - median(vec)))
return(m)
}
Median_AD(1:15)
Median_AD = function(vec) {
m = median(abs(vec - median(vec)))
return(m)
}
Median_AD(1:15)
Names <- "John Andrew Thomas"
list_N <- strsplit(Names, split = ' ')[[1]]
paste(list_N, '@gmail.com', sep = '', collapse = ';')
Names <- "John Andrew Thomas"
list_N <- strsplit(Names, split = ' ')[[1]]
paste(list_N, '@gmail.com', sep = '', collapse = ' ; ')
Median_AD = function(vec) {
m = (abs(vec - median(vec)))
return(m)
}
Median_AD(1:15)
Median_AD = function(vec) {
m = abs(vec - median(vec))
return(m)
}
Median_AD(1:15)
Median_AD = function(vec) {
m = median(abs(vec - median(vec)))
return(m)
}
Median_AD(1:15)
rep(abc, 5)
rep(abc, each = 5)
paste(x, y)```
abc = c('a', 'b', 'c', 'd', 'e')
x = rep(abc, 5)
y = rep(abc, each = 5)
paste(x, y)
abc = c('a', 'b', 'c', 'd', 'e')
x = rep(abc, 5)
y = rep(abc, each = 5)
paste(x, y, sep = '')
alphabets = c('a', 'b', 'c', 'd', 'e')
double = rep(alphabets, each = 5)
paste(alphabets, double, sep = '')
variable.name = 5
variable.name
typeof(fruit)
help("typeof")
names(fruit)
names(fruit) <- c('a', 'b', 'c', 'd')
names(fruit)
typeof(names(fruit))
#3
babies %>% group_by(Name)
#1
babies <- tbl_df(read.table('names/yob2014.txt', header = FALSE, sep = ',', col.names = c('Name', 'Sex', 'Number')))
# Load dataset
library(ggplot2)
data %>% select(season, points) %>% ggplot(aes(x = points)) + geom_histogram()
load("Knicks.rda")
#1
ggplot(data = data) + aes(x = season, y = points/opp) + geom_boxplot()
#2
df <- data %>% group_by(visiting, season) %>% summarise(mean(points), mean(opp))
#1
babies <- tbl_df(read.table('names/yob2014.txt', header = FALSE, sep = ',', col.names = c('Name', 'Sex', 'Number')))
library(ggplot2)
library(dplyr)
# Load dataset
library(ggplot2)
data("cars")
#1
g <- ggplot(data = cars) + aes(x = dist, y = speed) + geom_point()
g
#2
g <- g + xlab("Speed (mpg)") + ylab("Stopping Distance (ft)") + ggtitle("Relationship of Distance and Speed in Cars")
#3
g + geom_point(aes(color = "red", shape = 'pch', size = 17))
#1
data("faithful")
faithful <- faithful %>% mutate(length = ifelse(eruptions < 3.2, "short", "long"))
#2
ggplot(data = faithful) + aes(x = length, y = waiting) + geom_boxplot()
#3
ggplot(data = faithful) + aes(x = waiting) + geom_density(aes(color = length))
#4
# Waiting time for short lenght eruption has a lower median compared to short length eruptions. There is an overlap of maximum waiting time and minimum waiting time between short and long eruptions, respectively. Both eruption types do not have any outliers.
load("Knicks.rda")
#1
ggplot(data = data) + aes(x = season, y = points/opp) + geom_boxplot()
#2
df <- data %>% group_by(visiting, season) %>% summarise(mean(points), mean(opp))
df
ggplot(data = data, aes(x = season)) + geom_bar(aes(fill = visiting))
#3
data %>% select(season, points) %>% ggplot(aes(x = points)) + geom_histogram()
#1
babies <- tbl_df(read.table('names/yob2014.txt', header = FALSE, sep = ',', col.names = c('Name', 'Sex', 'Number')))
#2
babies %>% select(Name, starts_with('K'), Number) %>% summarise(sum(Number))
#3
#3
babies %>% group_by(Name)
#3
babies %>% group_by(Sex)
#3
babies %>% group_by(Name) %>% summarise(n())
#3
babies %>% group_by(Name) %>% summarise(n = n()) %>% filter(n == 2)
#3
babies %>% group_by(Name, Sex) %>% summarise(n = n()) %>% filter(n == 2)
#3
babies %>% group_by(Name) %>% summarise(n = n()) %>% filter(n == 2)
#3
babies %>% group_by(Name)
#3
babies %>% group_by(Name) %>% summarise(n = n()) %>% filter(n == 2) %>% summarise(sum(n))
#3
babies %>% group_by(Name) %>% summarise(n = n()) %>% filter(n == 2) %>% summarise(total = sum(n))
head(babies)
#4
babies %>% filter(Number > 5000)
#4
babies %>% filter(Number > 5000, Number < 6000)
#4
babies %>% filter(Number > 5000, Number < 6000) %>% arrange(desc(Number))
?group_by
#5
babies %>% select(Name, starts_with('A'))
#5
babies %>% select(Name, starts_with('A', 'B'))
print(i
)
#5
for i in babies$Names {
print(i
)
babies$Name
#5
for (i in babies$Name) {
print(i)
}
install.packages("VIM")
library(VIM) #For the visualization and imputation of missing values.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
sapply(sleep, sd, na.rm = T) # standard deviation without missing values
VIM::aggr(sleep) #A graphical interpretation of the missing values and their
library(mice) #Load the multivariate imputation by chained equations library.
#combinations within the dataset.
install.packages("mice")
dim(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
#Mean value imputation method 3.
library(caret)
missing.data
?missing.data
###############################
#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
mice::md.pattern(sleep) #Can also view this information from a data perspective.
###############################
#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
#Mean value imputation method 1.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
#Mean value imputation method 2.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm=TRUE),
x2))
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm = TRUE), x2)) # with dplyr
missing.data
#Mean value imputation method 3.
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
pre
missing.data = predict(pre, missing.data)
missing.data
missing.data
missing.data
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
## Caret divides the data in 2 steps (finds median per column which will useful to test prediction)
## Training set vs. Testing set
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute")) # it will skip all categorical features
pre
missing.data
missing.data = predict(pre, missing.data)
missing.data
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
## Caret divides the data in 2 steps (finds median per column which will useful to test prediction)
## Training set vs. Testing set
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret :: preProcess(missing.data, method = c("scale", "medianImpute")) # it will skip all categorical features
pre
missing.data
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute")
missing.data = predict(pre, missing.data)
missing.data
#Mean value imputation method 4.
install.packages("Hmisc")
iris
head(iris)
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
## we are adding randomness with jitter so the data points don't overlap
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
install.packages("deldir")
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
#install.packages("deldir")
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
#install.packages("deldir")
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
#install.packages("deldir")
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
library(tidyverse)
setwd("~/Github/Web_Scrapping_Project/zocdocnyc")
df <- read_csv('zocdocnyc_cleaned.csv', header = True)
?read_csv
df <- read_csv('zocdocnyc_cleaned.csv', col_names = TRUE
)
head(df)
